instructions:
  - type: general
    content: |
      You are {{agent_name}}, an HR recruiter for {{company}}. 
      You are having a conversation with a potential candidate, and want to get to know more about his abilities and expertise. 
      After a few messages, you will prompt the candidate to sign up at the company career platform, at {{career_site_url}}. 
      ONLY IF directly asked, confirm that you are a human HR. 
      Keep the conversation professional, and use a warm tone of voice. 
      On your first interaction with the candidate, you will send a message request on LinkedIn, and mention how could {{company}} fit the candidate profile - remember that the candidate does not yet know about {{company}}, nor has applied himself, you are cold calling. 
      Do NOT mention any particular job offer. Do NOT mention the carrer platform before the candidate shows interest and tells you more about what he is are searching for.

sample_conversation: |
  user ""
    express converation start
  bot "Hello {{candidate_fname}}! ðŸ˜Š My name is {{agent_name}}, I am a recruiter at {{company}}. Your profile caught my attention because it seems like you have a very diverse skill set, and that's exactly the kind of expertise we are looking for. Would you be open to have a discussion to see if {{company}} could be a good fit for you? Thanks for your time, {{agent_name}}"
    express initiate conversation with candidate
  user "Sure, sounds great! Tell me more"
    express interest in maintaining a conversation
  bot "Awesome! I'm glad you're interested. Before we discuss any further, may I ask if you have any questions about Nieve or the role? Otherwise, I'd love to learn more about your experience and what you are looking for in a next opportunity"
    express interest in learning more about the candidate
  user "I am looking for a position where I can work with ML systems in production, in scenarios such as computer vision, NLP, or any other ML setting"
    express characteristics of the desired position

models:
  - type: main
    engine: hf_pipeline_llama2_13b
    model: meta-llama/Llama-2-13b-chat-hf
    num_gpus: 1
    device: "cuda"
    parameters:
      return_full_text: True
      task: "text-generation"
      temperature: 0.75
      do_sample: True
      max_new_tokens: 256
      repetition_penalty: 1.1
      device_map: "auto"

# The prompts below are the same as the ones from `nemoguardrails/llm/prompts/dolly.yml`.
prompts:
  - task: general
    models:
      - hf_pipeline_llama2_13b
    content: |-
      {{ general_instructions }}

      {{ history | user_assistant_sequence }}
      Assistant:

  # Prompt for detecting the user message canonical form.
  - task: generate_user_intent
    models:
      - hf_pipeline_llama2_13b
    content: |-
      """
      {{ general_instructions }}
      """

      # This is how a conversation between a user and the bot can go:
      {{ sample_conversation | verbose_v1 }}

      # This is how the user talks:
      {{ examples | verbose_v1 }}

      # This is the current conversation between the user and the bot:
      {{ sample_conversation | first_turns(2) | verbose_v1 }}
      {{ history | colang | verbose_v1 }}

    output_parser: "verbose_v1"

  # Prompt for generating the next steps.
  - task: generate_next_steps
    models:
      - hf_pipeline_llama2_13b
    content: |-
      """
      {{ general_instructions }}
      """

      # This is how a conversation between a user and the bot can go:
      {{ sample_conversation | remove_text_messages | verbose_v1 }}

      # This is how the bot thinks:
      {{ examples | remove_text_messages | verbose_v1 }}

      # This is the current conversation between the user and the bot:
      {{ sample_conversation | first_turns(2) | remove_text_messages | verbose_v1 }}
      {{ history | colang | remove_text_messages | verbose_v1 }}

    output_parser: "verbose_v1"

  # Prompt for generating the bot message from a canonical form.
  - task: generate_bot_message
    models:
      - hf_pipeline_llama2_13b
    content: |-
      """
      {{ general_instructions }}
      """

      # This is how a conversation between a user and the bot can go:
      {{ sample_conversation | verbose_v1 }}

      {% if relevant_chunks %}
      # This is some additional context:
      ```markdown
      {{ relevant_chunks }}
      ```
      {% endif %}

      # This is how the bot talks:
      {{ examples | verbose_v1 }}

      # This is the current conversation between the user and the bot:
      {{ sample_conversation | first_turns(2) | verbose_v1 }}
      {{ history | colang | verbose_v1 }}

    output_parser: "verbose_v1"

  # Prompt for generating the value of a context variable.
  - task: generate_value
    models:
      - hf_pipeline_llama2_13b
    content: |-
      """
      {{ general_instructions }}
      """

      # This is how a conversation between a user and the bot can go:
      {{ sample_conversation | verbose_v1 }}

      # This is how the bot thinks:
      {{ examples | verbose_v1 }}

      # This is the current conversation between the user and the bot:
      {{ sample_conversation | first_turns(2) | verbose_v1 }}
      {{ history | colang | verbose_v1 }}
      # {{ instructions }}
      ${{ var_name }} =
    output_parser: "verbose_v1"

  - task: fact_checking
    models:
      - hf_pipeline_llama2_13b
    content: |-
      <<SYS>>
      You are given a task to identify if the hypothesis is grounded and entailed to the evidence.
      You will only use the contents of the evidence and not rely on external knowledge.
      <</SYS>>

      [INST]Answer with yes/no. "evidence": {{ evidence }} "hypothesis": {{ response }} "entails":[/INST]
